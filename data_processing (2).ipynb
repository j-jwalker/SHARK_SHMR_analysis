{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b6b15e-079b-466a-bf46-755ca11c2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Packages\n",
    "########################################\n",
    "import warnings\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde01640-89aa-4dc9-a71f-b1a3bb6de9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Functions and values\n",
    "########################################\n",
    "h0=0.6751    # h0 value used for the SURFS DM-only simulation used to generate these outputs (Planck+2015)\n",
    "Gyr2yr=1e9\n",
    "ncpu=128    # Number of cores, only important if reading the files in parallel (see near the end of the code)\n",
    "warnings.filterwarnings('ignore',message='invalid value encountered in divide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea728366-6e45-4e5b-9596-6b5cdfee8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHARK_parallel_read(shark_file,sv):\n",
    "    shark_hdf5=h5.File(shark_file)\n",
    "    \n",
    "    gal_sv={}\n",
    "    \n",
    "    # Shark properties\n",
    "    # Note the modification of galaxy/group IDs. This is because these IDs are only unique per subvolume, i.e.,\n",
    "    # you can have up to 64 galaxies with with the same ID (one per each simulation subvolume).\n",
    "    # If you want the true IDs it would be as simple as replacing these lines with something like\n",
    "    # gal_sv['ID_galaxy']=shark_hdf5['galaxies']['id_galaxy'][()]\n",
    "    gal_sv['ID_galaxy']=np.array([f'{sv}_{gid}' for gid in shark_hdf5['galaxies']['id_galaxy'][()]])\n",
    "    gal_sv['ID_subhalo']=np.array([f'{sv}_{hid}' for hid in shark_hdf5['galaxies']['id_subhalo'][()]])\n",
    "    gal_sv['ID_subhalo_tree']=np.array([f'{sv}_{hid}' for hid in shark_hdf5['galaxies']['id_subhalo_tree'][()]])\n",
    "    gal_sv['ID_halo']=np.array([f'{sv}_{hid}' for hid in shark_hdf5['galaxies']['id_halo'][()]])\n",
    "    gal_sv['sv']=(np.ones(len(shark_hdf5['galaxies']['id_galaxy'][()]))*sv).astype('int')\n",
    "    gal_sv['type']=shark_hdf5['galaxies']['type'][()]\n",
    "    # Halo masses\n",
    "    gal_sv['Mhalo_host']=shark_hdf5['galaxies']['mvir_hosthalo'][()].astype('float64')/h0\n",
    "    gal_sv['Mhalo']=shark_hdf5['galaxies']['mvir_subhalo'][()].astype('float64')/h0\n",
    "    # Stellar masses\n",
    "    gal_sv['Mstar_bulge']=shark_hdf5['galaxies']['mstars_bulge'][()].astype('float64')/h0\n",
    "    gal_sv['Mstar_disc']=shark_hdf5['galaxies']['mstars_disk'][()].astype('float64')/h0\n",
    "    gal_sv['Mstar_all']=gal_sv['Mstar_bulge']+gal_sv['Mstar_disc']\n",
    "    gal_sv['Mstar_stripped']=shark_hdf5['galaxies']['mstars_tidally_stripped'][()].astype('float64')/h0\n",
    "    # Gas masses\n",
    "    gal_sv['Mhotgas']=shark_hdf5['galaxies']['mhot'][()].astype('float64')/h0\n",
    "    gal_sv['Mcoldgas_bulge']=shark_hdf5['galaxies']['mgas_bulge'][()].astype('float64')/h0\n",
    "    gal_sv['Mcoldgas_disc']=shark_hdf5['galaxies']['mgas_disk'][()].astype('float64')/h0 \n",
    "    gal_sv['Mcoldgas_all']=gal_sv['Mcoldgas_bulge']+gal_sv['Mcoldgas_disc']\n",
    "    # Galaxy sizes\n",
    "    gal_sv['Rgas_bulge']=1e3*shark_hdf5['galaxies']['rgas_bulge'][()].astype('float64')/h0\n",
    "    gal_sv['Rgas_disc']=1e3*shark_hdf5['galaxies']['rgas_disk'][()].astype('float64')/h0\n",
    "    gal_sv['Rgas_all']=(gal_sv['Mstar_bulge']*gal_sv['Rgas_bulge']+gal_sv['Mstar_disc']*gal_sv['Rgas_disc'])/gal_sv['Mstar_all']\n",
    "    gal_sv['Rstar_bulge']=1e3*shark_hdf5['galaxies']['rstar_bulge'][()].astype('float64')/h0\n",
    "    gal_sv['Rstar_disc']=1e3*shark_hdf5['galaxies']['rstar_disk'][()].astype('float64')/h0\n",
    "    gal_sv['Rstar_all']=(gal_sv['Mstar_bulge']*gal_sv['Rstar_bulge']+gal_sv['Mstar_disc']*gal_sv['Rstar_disc'])/gal_sv['Mstar_all']\n",
    "    # SFR\n",
    "    gal_sv['SFR_bulge']=shark_hdf5['galaxies']['sfr_burst'][()].astype('float64')/(h0*Gyr2yr)\n",
    "    gal_sv['SFR_disc']=shark_hdf5['galaxies']['sfr_disk'][()].astype('float64')/(h0*Gyr2yr)\n",
    "    gal_sv['SFR_all']=gal_sv['SFR_bulge']+gal_sv['SFR_disc']\n",
    "    # BH properties\n",
    "    gal_sv['MBH']=shark_hdf5['galaxies']['m_bh'][()].astype('float64')/h0\n",
    "    gal_sv['MBHacc_cold']=shark_hdf5['galaxies']['bh_accretion_rate_sb'][()].astype('float64')/(h0*Gyr2yr)\n",
    "    gal_sv['MBHacc_hot']=shark_hdf5['galaxies']['bh_accretion_rate_hh'][()].astype('float64')/(h0*Gyr2yr)\n",
    "    gal_sv['MBHacc_all']=gal_sv['MBHacc_cold']+gal_sv['MBHacc_hot']\n",
    "    # Ejected gas\n",
    "    gal_sv['Mgas_lost_SF']=shark_hdf5['galaxies']['mreheated'][()].astype('float64')/h0\n",
    "    gal_sv['Mgas_lost_QSO']=shark_hdf5['galaxies']['mlost'][()].astype('float64')/h0\n",
    "    gal_sv['BHspin']=shark_hdf5['galaxies']['bh_spin'][()]\n",
    "    \n",
    "    # pandas DataFrame\n",
    "    gal_sv=pd.DataFrame(gal_sv)\n",
    "    gal_sv=gal_sv.drop(gal_sv[gal_sv['Mstar_all']<1e8].index)    # Galaxies below this mass tend to reside in poorly-resolved haloes\n",
    "    shark_hdf5.close()\n",
    "    \n",
    "    return(gal_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9826244a-0bef-4e40-a4d2-484c664db9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Reading data\n",
    "########################################\n",
    "sv_list=np.arange(64)\n",
    "sharkfiles=[f'extracted_data_z=0/199/{sv}/galaxies.hdf5' for sv in sv_list] \n",
    "\n",
    "# To read in parallel\n",
    "#pool=mp.Pool(processes=ncpu)\n",
    "#shark_csv=[pool.apply_async(SHARK_parallel_read,(sf,sv,)) for sf,sv in zip(sharkfiles,sv_list)]\n",
    "#shark_csv=[sc.get() for sc in shark_csv]\n",
    "#pool.close()\n",
    "\n",
    "# Otherwise use this\n",
    "shark_csv=[SHARK_parallel_read(sf,sv) for sf,sv in zip(sharkfiles,sv_list)] \n",
    "shark_csv=pd.concat(shark_csv,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
